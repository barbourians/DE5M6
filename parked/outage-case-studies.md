# Day 2: Major Outage Case Studies

---

## Overview
This investigation moves from the small-scale pipeline breaks learners just practiced to major real-world incidents affecting millions of users. By analysing how major companies detected, responded to, and communicated during significant outages, learners develop perspective on incident management at scale and identify best practices for their own operational contexts.

---

## Session Structure

### Setup & Assignment

#### Outage Assignment:
Each group gets assigned **one major outage** to research in depth:

**Outage 1: Facebook/Meta Global Outage (October 4, 2021)**

- All Facebook, Instagram, WhatsApp services down globally
- 6+ hour outage affecting 3+ billion users
- BGP routing configuration error

**Outage 2: AWS us-east-1 Outage (December 7, 2021)**

- Major AWS region failure affecting thousands of services
- Netflix, Ring, Amazon's own retail site affected
- Power-related issues in data center

**Outage 3: Google Cloud Global Outage (June 2, 2019)**

- YouTube, Gmail, Google Drive, Google Cloud Platform down
- Network congestion and configuration issues
- 4+ hour impact across multiple services

**Outage 4: GitHub Outage (October 21, 2018)**

- Major database failure affecting millions of developers
- 24+ hour degraded service
- Network partition and database replication issues

**Outage 5: Slack Global Outage (January 4, 2021)**

- Complete service unavailability during peak work hours
- DNS and internal infrastructure issues
- Major productivity impact for remote workers

**Outage 6: Microsoft 365 Global Outage (March 15, 2021)**

- Teams, Outlook, SharePoint affected globally
- Authentication system failures
- Major impact during pandemic remote work surge

---

### Investigation Phase (25 minutes)

#### Research Framework:
Use this structured investigation template:

### **Incident Timeline & Detection**

**When and How It Started:**

- What was the root cause? (technical failure, human error, external factor)
- How long before the company detected the problem?
- How did they find out? (monitoring alerts, user complaints, internal discovery)

**Timeline Research:**

- What time did the incident start?
- When did the company acknowledge it publicly?
- How long until full resolution?

### **Response and Communication**

**Internal Response:**

- Who was involved in the response? (on-call engineers, executives, PR teams)
- What was the escalation process?
- How did they coordinate across teams?

**External Communication:**

- How quickly did they communicate with users?
- What channels did they use? (status page, social media, email)
- How detailed/honest were their updates?

**Communication Examples:**

- Find 2-3 actual quotes from their status updates
- How did the tone/detail change over time?

### **Impact and Lessons**

**Business Impact:**

- How many users/customers affected?
- Estimated financial impact (if available)
- Reputation/trust impact

**Technical Lessons:**

- What did they say went wrong technically?
- What changes did they make to prevent recurrence?
- What processes/tools did they improve?

---

### Discussion Phase (25 minutes)

#### Report Back Structure:
Each group gets **5 minutes** to share key findings using this suggested format:

**The Incident:**

- What broke and how badly?
- How long did it take to fix?

**Detection & Response:**

- How did they find out?
- What went well or poorly in their response?

**Communication:**

- How did they communicate with users?
- What was effective or problematic?

**Lessons:**

- Key lesson for data operations teams?
- What would you do differently?

---
